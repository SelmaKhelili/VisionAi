<div align="center">

<br/>

<img src="docs/gallery.png" alt="VisionAI Gallery" width="800"/>

<br/><br/>

# VisionAI

**AI-powered image analysis dashboard, upload images and get automatic descriptions, tags, semantic search, and intelligent clustering.**

[![Python](https://img.shields.io/badge/Python-3.11-blue?style=flat-square&logo=python&logoColor=white)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-latest-009688?style=flat-square&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![React](https://img.shields.io/badge/React-18-61DAFB?style=flat-square&logo=react&logoColor=black)](https://react.dev)
[![CLIP](https://img.shields.io/badge/CLIP-ViT--B/32-orange?style=flat-square)](https://huggingface.co/openai/clip-vit-base-patch32)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow?style=flat-square)](LICENSE)

</div>

---

## âœ¨ Features

- ğŸ–¼ï¸ **Auto-analysis** â€” every uploaded image gets a natural language description, tags, mood, scene type, dominant colors, and time of day via Groq's vision LLM
- ğŸ” **Natural language search** â€” type "croissant", "peaceful lake", or "people outdoors" and find matching images instantly
- ğŸ§  **Semantic clustering** â€” images are grouped by meaning, not just visual style, using a hybrid CLIP + text embedding approach
- ğŸ“Š **Three views** â€” Gallery grid, Cluster groups, and Search results
- ğŸ¨ **Detail panel** â€” click any image for full AI analysis with all metadata
- ğŸ—‘ï¸ **Live management** â€” delete images and clusters re-compute automatically

---

## ğŸ–¥ï¸ Screenshots

<div align="center">

### Gallery
<img src="docs/gallery.png" alt="Gallery view" width="750"/>

<br/><br/>

### Clusters
<img src="docs/clusters.png" alt="Semantic clusters" width="750"/>

<br/><br/>

### Search
<img src="docs/search.png" alt="Natural language search" width="750"/>

</div>

---

## ğŸ—ï¸ Architecture

```
visionai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py          # FastAPI server, all API endpoints
â”‚   â”œâ”€â”€ vision.py        # Groq vision LLM â€” description, tags, mood
â”‚   â”œâ”€â”€ clustering.py    # Hybrid CLIP + text embedding + k-means
â”‚   â”œâ”€â”€ search.py        # Natural language search over descriptions
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env             # GROQ_API_KEY
â”‚
â””â”€â”€ frontend/
    â””â”€â”€ index.html       # Full React UI â€” single file, no build step
```

### How it works

```
Image uploaded
      â†“
Groq LLaMA 4 Scout (vision) â†’ description + tags + mood + scene
      â†“
Two embeddings computed in parallel:
  â”œâ”€â”€ CLIP ViT-B/32      â†’ 512-dim visual embedding (how it looks)
  â””â”€â”€ sentence-transformers â†’ 384-dim text embedding (what it means)
      â†“
Concatenated â†’ 896-dim hybrid feature vector
      â†“
K-means clustering across all images
      â†“
Gallery Â· Clusters Â· Search updated live
```

---

## ğŸ§  The Clustering Story

Getting clustering right took three iterations â€” each one teaching a real ML lesson.

**v1 â€” Color histograms**
Simple: extract RGB color distributions per image. Fast, zero dependencies.
Problem: two restaurants with different lighting ended up in different clusters
because their color histograms looked nothing alike.

**v2 â€” CLIP embeddings only**
CLIP understands visual semantics far better than color â€” it was trained on
400M image-text pairs. Restaurants clustered together regardless of lighting.
Problem: CLIP is so sensitive to photographic *style* that it split food into
"hero shots" vs "overhead spreads" â€” grouping pancakes with burgers because
both are centered on a plain background.

**v3 â€” Hybrid CLIP + text embeddings** âœ“
The fix: concatenate CLIP's visual embedding with a text embedding of the
AI-generated description. Now two signals work together:
- CLIP says "these look similar"
- Text says "these mean similar things"

Result: pancakes cluster with breakfast food. Restaurants cluster together
regardless of angle. Portraits form their own group. Much more intuitive.

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.11
- Conda (recommended on Windows)
- A free [Groq API key](https://console.groq.com)

### 1. Clone the repo

```bash
git clone https://github.com/yourusername/visionai.git
cd visionai
```

### 2. Create environment

```bash
conda create -n visionai python=3.11
conda activate visionai
```

### 3. Install dependencies

```bash
cd backend
pip install -r requirements.txt
```

> â³ First run downloads two models automatically:
> - CLIP ViT-B/32 (~600MB, cached forever after)
> - all-MiniLM-L6-v2 (~80MB, cached forever after)

### 4. Add your Groq API key

Edit `backend/.env`:

```env
GROQ_API_KEY=gsk_your_key_here
```

### 5. Run

```bash
python main.py
```

Open **http://localhost:8000** and start dropping images.

---

## ğŸ”¬ Tech Stack

| Component | Technology | Why |
|---|---|---|
| **Vision LLM** | Groq LLaMA 4 Scout | Multimodal model, fast structured JSON output |
| **Visual embeddings** | CLIP ViT-B/32 | Semantic visual understanding, 512-dim |
| **Text embeddings** | all-MiniLM-L6-v2 | Semantic text understanding, 384-dim |
| **Clustering** | scikit-learn K-Means | Simple, effective, no external DB |
| **Search** | Weighted keyword matching | Fast, works on AI-generated descriptions |
| **Backend** | FastAPI | Async, clean REST API |
| **Frontend** | React 18 (no build step) | Single HTML file |

---

## ğŸ’¡ Things I learned building this

- **Multimodal models** understand both images and text in the same embedding space â€” CLIP was trained by matching images with their captions, so it learns that a photo of a dog and the word "dog" should be close together.
- **Embeddings capture meaning, not pixels** â€” two images of the same scene from different angles, with different lighting, will still be close in CLIP's embedding space because they share semantic content.
- **Clustering is sensitive to feature choice** â€” using raw pixel values or color histograms clusters by appearance. Using CLIP clusters by visual semantics. Using text clusters by content. The best results come from blending both.
- **K-means needs the right k** â€” too few clusters and everything collapses together, too many and you get singleton groups. The `sqrt(n/2) + 1` heuristic is a practical starting point.
- **LLM output as features** â€” using the AI-generated description as an embedding input is an elegant trick: you're essentially letting the vision LLM do the semantic parsing, then encoding that understanding into vector form for the clustering algorithm.

---

## ğŸ“„ License

MIT

---

<div align="center">
  <sub>Built as Day 3 of a 30-day AI challenge ğŸš€</sub>
</div>
